---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: monitoring
spec:
  interval: 5m
  chart:
    spec:
      chart: kube-prometheus-stack
      version: 35.2.0
      sourceRef:
        kind: HelmRepository
        name: prometheus-community-charts
        namespace: flux-system
      interval: 5m
  install:
    createNamespace: true
    remediation:
      retries: 5
  upgrade:
    remediation:
      retries: 5
  values:
    fullnameOverride: prom-stack
    nodeExporter:
    tolerations:
      - key: "thin-client"
        operator: "Exists"
        effect: "NoExecute"
    alertmanager:
      config:
        global:
          resolve_timeout: 5m
        # receivers:
        #   - name: "null"
        #   - name: "pushover"
        #     pushover_configs:
        #       - token: "${SECRET_PUSHOVER_ALERT_MANAGER_APIKEY}"
        #         user_key: "${SECRET_PUSHOVER_USERKEY}"
        #         send_resolved: true
        #         html: true
        #         priority: |-
        #           {{ if eq .Status "firing" }}1{{ else }}0{{ end }}
        #         url_title: View in Alert Manager
        #         title: |-
        #           [{{ .Status | toUpper -}}
        #           {{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{- end -}}
        #           ] {{ .CommonLabels.alertname }}
        #         message: |-
        #           {{- range .Alerts }}
        #             {{- if ne .Labels.severity "" }}
        #               <b>Severity:</b> <i>{{ .Labels.severity }}</i>
        #             {{- else }}
        #               <b>Severity:</b> <i>N/A</i>
        #             {{- end }}
        #             {{- if ne .Annotations.description "" }}
        #               <b>Description:</b> <i>{{ .Annotations.description }}</i>
        #             {{- else if ne .Annotations.summary "" }}
        #               <b>Summary:</b> <i>{{ .Annotations.summary }}</i>
        #             {{- else if ne .Annotations.message "" }}
        #               <b>Message:</b> <i>{{ .Annotations.message }}</i>
        #             {{- else }}
        #               <b>Description:</b> <i>N/A</i>
        #             {{- end }}
        #             {{- if gt (len .Labels.SortedPairs) 0 }}
        #               <b>Details:</b>
        #               {{- range .Labels.SortedPairs }}
        #                 â€¢ <b>{{ .Name }}:</b> <i>{{ .Value }}</i>
        #               {{- end }}
        #             {{- end }}
        #           {{- end }}
        #   - name: "nodered"
        #     webhook_configs:
        #       - url: "http://node-red.home:1880/power-outage"
        #         send_resolved: true
        # route:
        #   group_by: ["alertname", "job"]
        #   group_wait: 30s
        #   group_interval: 5m
        #   repeat_interval: 6h
        #   receiver: "pushover"
        #   routes:
        #     - receiver: "null"
        #       matchers:
        #         - alertname =~ "InfoInhibitor|Watchdog"
        #     - receiver: "pushover"
        #       matchers:
        #         - severity = "critical"
        #       continue: true
        #     - receiver: "nodered"
        #       matchers:
        #         - alertname = "UPSOnBattery"
        #       continue: true
        inhibit_rules:
          - source_matchers:
              - severity = "critical"
            target_matchers:
              - severity = "warning"
            equal: ["alertname", "namespace"]
      ingress:
        enabled: true
        pathType: Prefix
        ingressClassName: "traefik"
        annotations:
          traefik.ingress.kubernetes.io/router.entrypoints: "websecure"
          traefik.ingress.kubernetes.io/router.middlewares: "networking-internal-only@kubernetescrd"
        hosts:
          - "alert-manager.${SECRET_DOMAIN}"
      alertmanagerSpec:
        replicas: 2
        podAntiAffinity: hard
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: "cstor-csi-disk"
              resources:
                requests:
                  storage: 250Mi
            metadata:
              name: alert
    kube-state-metrics:
      metricLabelsAllowlist:
        - "persistentvolumeclaims=[*]"
      prometheus:
        monitor:
          enabled: true
          relabelings:
            - action: replace
              regex: (.*)
              replacement: $1
              sourceLabels:
                - __meta_kubernetes_pod_node_name
              targetLabel: kubernetes_node
    grafana:
      enabled: false
      forceDeployDashboards: true
      sidecar:
        dashboards:
          multicluster:
            etcd:
              enabled: true
    kubelet:
      enabled: true
      serviceMonitor:
        metricRelabelings:
          - action: replace
            sourceLabels:
              - node
            targetLabel: instance
    kubeApiServer:
      enabled: true
    kubeControllerManager:
      enabled: true
      endpoints:
        - 192.168.2.85
        - 192.168.2.86
        - 192.168.2.77
      service:
        enabled: true
        port: 10257
        targetPort: 10257
      serviceMonitor:
        enabled: true
        https: true
        insecureSkipVerify: true
    kubeScheduler:
      enabled: true
      endpoints:
        - 192.168.2.85
        - 192.168.2.86
        - 192.168.2.77
      service:
        enabled: true
        port: 10259
        targetPort: 10259
      serviceMonitor:
        enabled: true
        https: true
        insecureSkipVerify: true
    kubeProxy:
      enabled: true
      endpoints:
        - 192.168.2.85
        - 192.168.2.86
        - 192.168.2.77
    kubeEtcd:
      enabled: true
      endpoints:
        - 192.168.2.85
        - 192.168.2.86
        - 192.168.2.77
      service:
        enabled: true
        port: 2381
        targetPort: 2381
    prometheus:
      ingress:
        enabled: true
        pathType: Prefix
        ingressClassName: "traefik"
        annotations:
          traefik.ingress.kubernetes.io/router.entrypoints: "websecure"
          traefik.ingress.kubernetes.io/router.middlewares: "networking-internal-only@kubernetescrd"
        hosts:
          - "prometheus.${SECRET_DOMAIN}"
      # thanosService:
      #   enabled: true
      # thanosServiceMonitor:
      #   enabled: true
      # thanosIngress:
      #   enabled: true
      #   pathType: Prefix
      #   ingressClassName: "traefik"
      #   annotations:
      #     traefik.ingress.kubernetes.io/router.entrypoints: "websecure"
      #     traefik.ingress.kubernetes.io/router.middlewares: "networking-internal-only@kubernetescrd"
      #   hosts:
      #     - "thanos-sidecar.${SECRET_DOMAIN}"
      prometheusSpec:
        replicas: 1
        replicaExternalLabelName: "replica"
        podAntiAffinity: hard
        retentionSize: "6GB"
        ruleSelectorNilUsesHelmValues: false
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        probeSelectorNilUsesHelmValues: false
        retention: 2d
        enableAdminAPI: true
        walCompression: true
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: "cstor-csi-disk"
              resources:
                requests:
                  storage: 10Gi
            metadata:
              name: prom
        # thanos:
        #   image: quay.io/thanos/thanos:v0.26.0
        #   # renovate: datasource=docker depName=quay.io/thanos/thanos
        #   version: "v0.26.0"
        #   objectStorageConfig:
        #     name: thanos-objstore-secret
        #     key: objstore.yml
        resources:
          requests:
            cpu: 500m
            memory: 2000Mi
          limits:
            memory: 8000Mi
        # additionalScrapeConfigs:
        #   - job_name: coredns
        #     scrape_interval: 1m
        #     scrape_timeout: 10s
        #     honor_timestamps: true
        #     static_configs:
        #       - targets:
        #           - "opnsense.${SECRET_PRIVATE_DOMAIN}:9153"
        #   - job_name: node-exporter
        #     scrape_interval: 1m
        #     scrape_timeout: 10s
        #     honor_timestamps: true
        #     static_configs:
        #       - targets:
        #           - "opnsense.${SECRET_PRIVATE_DOMAIN}:9100"
        #           - "expanse.${SECRET_PRIVATE_DOMAIN}:9100"
        #           - "valetudo.${SECRET_PRIVATE_DOMAIN}:9100"
        #           - "pikvm.${SECRET_PRIVATE_DOMAIN}:9100"
        #   - job_name: pikvm
        #     scrape_interval: 1m
        #     scrape_timeout: 10s
        #     metrics_path: /api/export/prometheus/metrics
        #     basic_auth:
        #       username: "${SECRET_PIKVM_USERNAME}"
        #       password: "${SECRET_PIKVM_PASSWORD}"
        #     static_configs:
        #       - targets:
        #           - "pikvm.${SECRET_PRIVATE_DOMAIN}"
        #   - job_name: traefik
        #     scrape_interval: 1m
        #     scrape_timeout: 10s
        #     honor_timestamps: true
        #     static_configs:
        #       - targets:
        #           - "expanse.${SECRET_PRIVATE_DOMAIN}:7071"
        #   - job_name: nexus
        #     scrape_interval: 1m
        #     scrape_timeout: 10s
        #     metrics_path: /service/metrics/prometheus
        #     basic_auth:
        #       username: "${SECRET_NEXUS_USERNAME}"
        #       password: "${SECRET_NEXUS_PASSWORD}"
        #     static_configs:
        #       - targets:
        #           - "expanse.${SECRET_PRIVATE_DOMAIN}:8081"
